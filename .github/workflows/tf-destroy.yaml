name: "Terraform Destroy"

on:
  workflow_dispatch:
    inputs:
      cluster_region:
        description: cluster_region description
        required: true
      cluster_name:
        description: cluster_name description
        required: true
      cluster_env:
        description: cluster_env description
        required: true

env:
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}  
  TF_STATE_NAME: $GITHUB_REF # or ${{ github.ref }} ?????
    
jobs:
  terraform-destroy:
    name: "Terraform destroy"
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      
      - name: Install Terraform
        env:
          TERRAFORM_VERSION: "0.13.0"
        run: |
          tf_version=$TERRAFORM_VERSION
          wget https://releases.hashicorp.com/terraform/"$tf_version"/terraform_"$tf_version"_linux_amd64.zip
          unzip terraform_"$tf_version"_linux_amd64.zip
          sudo mv terraform /usr/local/bin/
      
      - name: Terraform Destroy
        if: github.ref == 'refs/heads/master' && github.event_name == 'push' # if: $CI_PIPELINE_SOURCE != "merge_request_event" ; when: manual
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client
          
          curl -s https://fluxcd.io/install.sh | bash
          flux -v
          
          terraform init
          terraform output -json > tf_output.json
          
          export cluster_region=${{ github.event.inputs.cluster_region }}
          export cluster_name=${{ github.event.inputs.cluster_name }}
          export cluster_env=${{ github.event.inputs.cluster_env }}
          
          export cluster_exists=$(aws eks --region ${cluster_region} list-clusters | grep -c ${cluster_name})
          |
            if [[ ${cluster_exists} == 1 ]]
            then 
              aws eks --region ${cluster_region} update-kubeconfig \
                --name ${cluster_name} \
                --alias eks-cluster
            fi
          |
            if [[ ${cluster_exists} == 1 ]]
            then 
              flux uninstall -s --keep-namespace
            # elif [[ $(terraform state list | grep -c kubernetes_namespace.flux_system) != "0" ]]
            # then
            #   terraform state rm kubernetes_namespace.flux_system
            fi
          |
            aws ec2 --region ${cluster_region} describe-instances \
              --filters Name=instance-state-name,Values=running \
              --filter Name=tag:kubernetes.io/cluster/${cluster_name},Values=owned \
              --filters Name=tag-key,Values=karpenter.sh/provisioner-name \
              --query "Reservations[*].Instances[*].InstanceId" \
              --output text | xargs aws ec2 --region ${cluster_region} terminate-instances --instance-ids || true
          |
            aws resourcegroupstaggingapi --region ${cluster_region} get-resources \
              --tag-filters Key=Environment,Values="${cluster_env}" Key=Project,Values="${cluster_name}" \
              --resource-type-filters 'dynamodb' \
              --query "ResourceTagMappingList[*].ResourceARN" \
              --output text | jq -Rc 'split("/")[1]' | xargs aws dynamodb --region ${cluster_region} delete-table --table-name || true
          terraform destroy -var-file=config/${TF_STATE_NAME}.tfvars